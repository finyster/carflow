# main_visualization.py â€“ v13.2 (å„²å­˜è»Šç¨®è³‡è¨Š)
import os, time, yaml, cv2, json
import numpy as np
from collections import defaultdict
from ultralytics import YOLO
from deep_sort_realtime.deepsort_tracker import DeepSort
from tqdm import tqdm

def get_color(cls):
    pal = {"car": (0, 255, 255), "bus": (255, 0, 0), "truck": (0, 0, 255), "motorcycle": (0, 255, 0)}
    return pal.get(cls, (200, 200, 200))

def main():
    try:
        with open("config.yaml") as f: cfg = yaml.safe_load(f)
    except FileNotFoundError: print("âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° config.yaml è¨­å®šæª”ï¼"); return

    video_path = cfg.get("video_path")
    model_path = cfg.get("model_path", "yolov8s.pt")
    output_video_path = cfg.get("save_video", "results/annotated_tracks.mp4")
    target_classes = cfg.get("classes", ["car", "bus", "truck", "motorcycle"])
    
    print("åˆå§‹åŒ–æ¨¡å‹..."); model = YOLO(model_path)
    tracker = DeepSort(max_age=50, n_init=3, nms_max_overlap=1.0)

    cap = cv2.VideoCapture(video_path)
    W, H, FPS = (int(cap.get(p)) for p in [cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS])
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), FPS or 30, (W, H))

    # âœ¨ã€æ ¸å¿ƒä¿®æ”¹ 1ã€‘âœ¨ å»ºç«‹ä¸€å€‹æ–°çš„è³‡æ–™çµæ§‹ä¾†å„²å­˜è»Šç¨®å’Œè»Œè·¡
    vehicle_tracks = defaultdict(lambda: {"class": None, "track": []})
    
    print(f"ğŸš€ é–‹å§‹ç¹ªè£½ä¸¦æ”¶é›†è»Œè·¡: {video_path}")
    
    try:
        for _ in tqdm(range(total_frames), desc="å½±ç‰‡è™•ç†é€²åº¦"):
            ret, frame = cap.read()
            if not ret: break

            results = model(frame, verbose=False, conf=0.3, imgsz=1280)
            detections = []
            for box in results[0].boxes:
                cls_name = model.names[int(box.cls[0])]
                if cls_name in target_classes:
                    x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())
                    conf = float(box.conf[0])
                    detections.append(([x1, y1, x2 - x1, y2 - y1], conf, cls_name))

            tracks = tracker.update_tracks(detections, frame=frame)
            
            for track in tracks:
                if not track.is_confirmed() or track.time_since_update > 1: continue
                track_id = track.track_id; cls_name = track.get_det_class(); ltrb = track.to_ltrb()
                x1, y1, x2, y2 = map(int, ltrb)
                cx, cy = (x1 + x2) // 2, y2
                
                # âœ¨ã€æ ¸å¿ƒä¿®æ”¹ 2ã€‘âœ¨ å°‡è»Šç¨®å’Œè»Œè·¡é»åŒæ™‚å­˜å…¥æ–°çš„è³‡æ–™çµæ§‹
                info = vehicle_tracks[track_id]
                if info["class"] is None:
                    info["class"] = cls_name
                info["track"].append((cx, cy))
                
                color = get_color(cls_name)
                label = f'{cls_name} ID:{track_id}'
                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
                
                pts = np.array(info["track"], np.int32).reshape((-1, 1, 2))
                cv2.polylines(frame, [pts], isClosed=False, color=color, thickness=2)
            
            out.write(frame)

    finally:
        print("è³‡æºé‡‹æ”¾èˆ‡å­˜æª”..."); cap.release(); out.release(); cv2.destroyAllWindows()
        print(f"ğŸ‰ è»Œè·¡ç¹ªè£½å®Œæˆï¼å½±ç‰‡å·²å„²å­˜è‡³: {output_video_path}")

        # âœ¨ã€æ ¸å¿ƒä¿®æ”¹ 3ã€‘âœ¨ å°‡æ–°çš„ã€æ›´è±å¯Œçš„è³‡æ–™çµæ§‹å¯«å…¥ JSON æª”æ¡ˆ
        trajectory_file_path = "results/trajectories.json"
        with open(trajectory_file_path, 'w') as f:
            json.dump(vehicle_tracks, f, indent=4)
        print(f"ğŸ“ˆ å¸¶æœ‰è»Šç¨®è³‡è¨Šçš„å®Œæ•´è»Œè·¡æ•¸æ“šå·²å„²å­˜è‡³: {trajectory_file_path}")

if __name__ == "__main__":
    main()